{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==0.25.3\n",
      "  Using cached pandas-0.25.3-cp36-cp36m-win_amd64.whl (9.0 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from pandas==0.25.3) (2.8.1)\n",
      "Collecting numpy>=1.13.3\n",
      "  Downloading numpy-1.19.3-cp36-cp36m-win_amd64.whl (13.2 MB)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from python-dateutil>=2.6.1->pandas==0.25.3) (1.15.0)\n",
      "Installing collected packages: numpy, pytz, pandas\n",
      "Successfully installed numpy-1.19.3 pandas-0.25.3 pytz-2020.1\n",
      "Collecting numpy==1.17.3\n",
      "  Downloading numpy-1.17.3-cp36-cp36m-win_amd64.whl (12.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.3\n",
      "    Uninstalling numpy-1.19.3:\n",
      "      Successfully uninstalled numpy-1.19.3\n",
      "Successfully installed numpy-1.17.3\n",
      "Collecting Keras==2.3.1\n",
      "  Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting scipy>=0.14\n",
      "  Downloading scipy-1.5.3-cp36-cp36m-win_amd64.whl (31.2 MB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from Keras==2.3.1) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from Keras==2.3.1) (1.17.3)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.0.0-cp36-cp36m-win_amd64.whl (2.7 MB)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-5.3.1-cp36-cp36m-win_amd64.whl (215 kB)\n",
      "Collecting cached-property\n",
      "  Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: keras-preprocessing, cached-property, h5py, keras-applications, scipy, pyyaml, Keras\n",
      "Successfully installed Keras-2.3.1 cached-property-1.5.2 h5py-3.0.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 pyyaml-5.3.1 scipy-1.5.3\n",
      "Collecting tensorflow==2.0.0\n",
      "  Using cached tensorflow-2.0.0-cp36-cp36m-win_amd64.whl (48.1 MB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from tensorflow==2.0.0) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from tensorflow==2.0.0) (1.0.8)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "  Using cached tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.33.2-cp36-cp36m-win_amd64.whl (2.7 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "  Using cached tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
      "Processing c:\\users\\win10\\appdata\\local\\pip\\cache\\wheels\\93\\2a\\eb\\e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\\termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Processing c:\\users\\win10\\appdata\\local\\pip\\cache\\wheels\\32\\42\\7f\\23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\\wrapt-1.12.1-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from tensorflow==2.0.0) (1.17.3)\n",
      "Processing c:\\users\\win10\\appdata\\local\\pip\\cache\\wheels\\19\\a7\\b9\\0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\\gast-0.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from tensorflow==2.0.0) (0.35.1)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Using cached protobuf-3.13.0-cp36-cp36m-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from tensorflow==2.0.0) (1.15.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (3.0.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (50.3.0.post20201006)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: cached-property in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.0.0) (1.5.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.0.0)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2020.6.20)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.4.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: google-pasta, absl-py, tensorflow-estimator, grpcio, opt-einsum, markdown, chardet, idna, urllib3, requests, werkzeug, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, protobuf, tensorboard, termcolor, astor, wrapt, gast, tensorflow\n",
      "Successfully installed absl-py-0.11.0 astor-0.8.1 cachetools-4.1.1 chardet-3.0.4 gast-0.2.2 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.33.2 idna-2.10 markdown-3.3.3 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 termcolor-1.1.0 urllib3-1.25.11 werkzeug-1.0.1 wrapt-1.12.1\n",
      "Collecting tqdm==4.43.0\n",
      "  Using cached tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.43.0\n",
      "Collecting matplotlib==3.1.3\n",
      "  Downloading matplotlib-3.1.3-cp36-cp36m-win_amd64.whl (9.1 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.2.0-cp36-none-win_amd64.whl (57 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from matplotlib==3.1.3) (2.8.1)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from matplotlib==3.1.3) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from matplotlib==3.1.3) (1.17.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
      "Installing collected packages: kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==0.25.3\n",
    "!pip install numpy==1.17.3\n",
    "!pip install Keras==2.3.1\n",
    "!pip install tensorflow==2.0.0\n",
    "!pip install tqdm==4.43.0\n",
    "!pip install matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def create_unique_word_dict(text:list) -> dict:\n",
    "    \"\"\"\n",
    "    A method that creates a dictionary where the keys are unique words\n",
    "    and key values are indices\n",
    "    \"\"\"\n",
    "    # Getting all the unique words from our text and sorting them alphabetically\n",
    "    words = list(set(text))\n",
    "    words.sort()\n",
    "\n",
    "    # Creating the dictionary for the unique words\n",
    "    unique_word_dict = {}\n",
    "    for i, word in enumerate(words):\n",
    "        unique_word_dict.update({\n",
    "            word: i\n",
    "        })\n",
    "    print(unique_word_dict)\n",
    "    return unique_word_dict    \n",
    "\n",
    "def text_preprocessing(\n",
    "    text:list,\n",
    "    punctuations = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_“~''',\n",
    "    stop_words=['and', 'a', 'is', 'the', 'in', 'be', 'will']\n",
    "    )->list:\n",
    "    \"\"\"\n",
    "    A method to preproces text\n",
    "    \"\"\"\n",
    "    for x in text.lower(): \n",
    "        if x in punctuations: \n",
    "            text = text.replace(x, \"\")\n",
    "\n",
    "    # Removing words that have numbers in them\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "\n",
    "    # Removing digits\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Setting every word to lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Converting all our text to a list \n",
    "    text = text.split(' ')\n",
    "\n",
    "    # Droping empty strings\n",
    "    text = [x for x in text if x!='']\n",
    "\n",
    "    # Droping stop words\n",
    "    text = [x for x in text if x not in stop_words]\n",
    "\n",
    "    return text\n",
    "\n",
    "# Functions to find the most similar word \n",
    "def euclidean(vec1:np.array, vec2:np.array) -> float:\n",
    "    \"\"\"\n",
    "    A function to calculate the euclidean distance between two vectors\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((vec1 - vec2)**2))\n",
    "\n",
    "def find_similar(word:str, embedding_dict:dict, top_n=10)->list:\n",
    "    \"\"\"\n",
    "    A method to find the most similar word based on the learnt embeddings\n",
    "    \"\"\"\n",
    "    dist_dict = {}\n",
    "    word_vector = embedding_dict.get(word, [])\n",
    "    if len(word_vector) > 0:\n",
    "        for key, value in embedding_dict.items():\n",
    "            if key!=word:\n",
    "                dist = euclidean(word_vector, value)\n",
    "                dist_dict.update({\n",
    "                    key: dist\n",
    "                })\n",
    "\n",
    "        return sorted(dist_dict.items(), key=lambda x: x[1])[0:top_n]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:00, 8164.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['future', 'king', 'prince']\n",
      "......i..... 0 ....word.... future\n",
      "......i..... 1 ....word.... king\n",
      "......i..... 2 ....word.... prince\n",
      "['daughter', 'princess']\n",
      "......i..... 0 ....word.... daughter\n",
      "......i..... 1 ....word.... princess\n",
      "['son', 'prince']\n",
      "......i..... 0 ....word.... son\n",
      "......i..... 1 ....word.... prince\n",
      "['only', 'man', 'can', 'king']\n",
      "......i..... 0 ....word.... only\n",
      "......i..... 1 ....word.... man\n",
      "......i..... 2 ....word.... can\n",
      "......i..... 3 ....word.... king\n",
      "['only', 'woman', 'can', 'queen']\n",
      "......i..... 0 ....word.... only\n",
      "......i..... 1 ....word.... woman\n",
      "......i..... 2 ....word.... can\n",
      "......i..... 3 ....word.... queen\n",
      "['princess', 'queen']\n",
      "......i..... 0 ....word.... princess\n",
      "......i..... 1 ....word.... queen\n",
      "['queen', 'king', 'rule', 'realm']\n",
      "......i..... 0 ....word.... queen\n",
      "......i..... 1 ....word.... king\n",
      "......i..... 2 ....word.... rule\n",
      "......i..... 3 ....word.... realm\n",
      "['prince', 'strong', 'man']\n",
      "......i..... 0 ....word.... prince\n",
      "......i..... 1 ....word.... strong\n",
      "......i..... 2 ....word.... man\n",
      "['princess', 'beautiful', 'woman']\n",
      "......i..... 0 ....word.... princess\n",
      "......i..... 1 ....word.... beautiful\n",
      "......i..... 2 ....word.... woman\n",
      "['royal', 'family', 'king', 'queen', 'their', 'children']\n",
      "......i..... 0 ....word.... royal\n",
      "......i..... 1 ....word.... family\n",
      "......i..... 2 ....word.... king\n",
      "......i..... 3 ....word.... queen\n",
      "......i..... 4 ....word.... their\n",
      "......i..... 5 ....word.... children\n",
      "['prince', 'only', 'boy', 'now']\n",
      "......i..... 0 ....word.... prince\n",
      "......i..... 1 ....word.... only\n",
      "......i..... 2 ....word.... boy\n",
      "......i..... 3 ....word.... now\n",
      "['boy', 'man']\n",
      "......i..... 0 ....word.... boy\n",
      "......i..... 1 ....word.... man\n",
      "word list  [['future', 'king'], ['future', 'prince'], ['king', 'prince'], ['king', 'future'], ['prince', 'king'], ['prince', 'future'], ['daughter', 'princess'], ['princess', 'daughter'], ['son', 'prince'], ['prince', 'son'], ['only', 'man'], ['only', 'can'], ['only', 'king'], ['man', 'can'], ['man', 'only'], ['man', 'king'], ['can', 'king'], ['can', 'man'], ['can', 'only'], ['king', 'can'], ['king', 'man'], ['king', 'only'], ['only', 'woman'], ['only', 'can'], ['only', 'queen'], ['woman', 'can'], ['woman', 'only'], ['woman', 'queen'], ['can', 'queen'], ['can', 'woman'], ['can', 'only'], ['queen', 'can'], ['queen', 'woman'], ['queen', 'only'], ['princess', 'queen'], ['queen', 'princess'], ['queen', 'king'], ['queen', 'rule'], ['queen', 'realm'], ['king', 'rule'], ['king', 'queen'], ['king', 'realm'], ['rule', 'realm'], ['rule', 'king'], ['rule', 'queen'], ['realm', 'rule'], ['realm', 'king'], ['realm', 'queen'], ['prince', 'strong'], ['prince', 'man'], ['strong', 'man'], ['strong', 'prince'], ['man', 'strong'], ['man', 'prince'], ['princess', 'beautiful'], ['princess', 'woman'], ['beautiful', 'woman'], ['beautiful', 'princess'], ['woman', 'beautiful'], ['woman', 'princess'], ['royal', 'family'], ['royal', 'king'], ['royal', 'queen'], ['family', 'king'], ['family', 'royal'], ['family', 'queen'], ['family', 'their'], ['king', 'queen'], ['king', 'family'], ['king', 'their'], ['king', 'royal'], ['king', 'children'], ['queen', 'their'], ['queen', 'king'], ['queen', 'children'], ['queen', 'family'], ['queen', 'royal'], ['their', 'children'], ['their', 'queen'], ['their', 'king'], ['their', 'family'], ['children', 'their'], ['children', 'queen'], ['children', 'king'], ['prince', 'only'], ['prince', 'boy'], ['prince', 'now'], ['only', 'boy'], ['only', 'prince'], ['only', 'now'], ['boy', 'now'], ['boy', 'only'], ['boy', 'prince'], ['now', 'boy'], ['now', 'only'], ['now', 'prince'], ['boy', 'man'], ['man', 'boy']]\n",
      "{'beautiful': 0, 'boy': 1, 'can': 2, 'children': 3, 'daughter': 4, 'family': 5, 'future': 6, 'king': 7, 'man': 8, 'now': 9, 'only': 10, 'prince': 11, 'princess': 12, 'queen': 13, 'realm': 14, 'royal': 15, 'rule': 16, 'son': 17, 'strong': 18, 'their': 19, 'woman': 20}\n",
      "unique_word_dict {'beautiful': 0, 'boy': 1, 'can': 2, 'children': 3, 'daughter': 4, 'family': 5, 'future': 6, 'king': 7, 'man': 8, 'now': 9, 'only': 10, 'prince': 11, 'princess': 12, 'queen': 13, 'realm': 14, 'royal': 15, 'rule': 16, 'son': 17, 'strong': 18, 'their': 19, 'woman': 20}\n",
      "words ['beautiful', 'boy', 'can', 'children', 'daughter', 'family', 'future', 'king', 'man', 'now', 'only', 'prince', 'princess', 'queen', 'realm', 'royal', 'rule', 'son', 'strong', 'their', 'woman']\n",
      "......i..... 0 ....word_list.... ['future', 'king'] word_list[0] future word_list[1]  king\n",
      "......i..... 1 ....word_list.... ['future', 'prince'] word_list[0] future word_list[1]  prince\n",
      "......i..... 2 ....word_list.... ['king', 'prince'] word_list[0] king word_list[1]  prince\n",
      "......i..... 3 ....word_list.... ['king', 'future'] word_list[0] king word_list[1]  future\n",
      "......i..... 4 ....word_list.... ['prince', 'king'] word_list[0] prince word_list[1]  king\n",
      "......i..... 5 ....word_list.... ['prince', 'future'] word_list[0] prince word_list[1]  future\n",
      "......i..... 6 ....word_list.... ['daughter', 'princess'] word_list[0] daughter word_list[1]  princess\n",
      "......i..... 7 ....word_list.... ['princess', 'daughter'] word_list[0] princess word_list[1]  daughter\n",
      "......i..... 8 ....word_list.... ['son', 'prince'] word_list[0] son word_list[1]  prince\n",
      "......i..... 9 ....word_list.... ['prince', 'son'] word_list[0] prince word_list[1]  son\n",
      "......i..... 10 ....word_list.... ['only', 'man'] word_list[0] only word_list[1]  man\n",
      "......i..... 11 ....word_list.... ['only', 'can'] word_list[0] only word_list[1]  can\n",
      "......i..... 12 ....word_list.... ['only', 'king'] word_list[0] only word_list[1]  king\n",
      "......i..... 13 ....word_list.... ['man', 'can'] word_list[0] man word_list[1]  can\n",
      "......i..... 14 ....word_list.... ['man', 'only'] word_list[0] man word_list[1]  only\n",
      "......i..... 15 ....word_list.... ['man', 'king'] word_list[0] man word_list[1]  king\n",
      "......i..... 16 ....word_list.... ['can', 'king'] word_list[0] can word_list[1]  king\n",
      "......i..... 17 ....word_list.... ['can', 'man'] word_list[0] can word_list[1]  man\n",
      "......i..... 18 ....word_list.... ['can', 'only'] word_list[0] can word_list[1]  only\n",
      "......i..... 19 ....word_list.... ['king', 'can'] word_list[0] king word_list[1]  can\n",
      "......i..... 20 ....word_list.... ['king', 'man'] word_list[0] king word_list[1]  man\n",
      "......i..... 21 ....word_list.... ['king', 'only'] word_list[0] king word_list[1]  only\n",
      "......i..... 22 ....word_list.... ['only', 'woman'] word_list[0] only word_list[1]  woman\n",
      "......i..... 23 ....word_list.... ['only', 'can'] word_list[0] only word_list[1]  can\n",
      "......i..... 24 ....word_list.... ['only', 'queen'] word_list[0] only word_list[1]  queen\n",
      "......i..... 25 ....word_list.... ['woman', 'can'] word_list[0] woman word_list[1]  can\n",
      "......i..... 26 ....word_list.... ['woman', 'only'] word_list[0] woman word_list[1]  only\n",
      "......i..... 27 ....word_list.... ['woman', 'queen'] word_list[0] woman word_list[1]  queen\n",
      "......i..... 28 ....word_list.... ['can', 'queen'] word_list[0] can word_list[1]  queen\n",
      "......i..... 29 ....word_list.... ['can', 'woman'] word_list[0] can word_list[1]  woman\n",
      "......i..... 30 ....word_list.... ['can', 'only'] word_list[0] can word_list[1]  only\n",
      "......i..... 31 ....word_list.... ['queen', 'can'] word_list[0] queen word_list[1]  can\n",
      "......i..... 32 ....word_list.... ['queen', 'woman'] word_list[0] queen word_list[1]  woman\n",
      "......i..... 33 ....word_list.... ['queen', 'only'] word_list[0] queen word_list[1]  only\n",
      "......i..... 34 ....word_list.... ['princess', 'queen'] word_list[0] princess word_list[1]  queen\n",
      "......i..... 35 ....word_list.... ['queen', 'princess'] word_list[0] queen word_list[1]  princess\n",
      "......i..... 36 ....word_list.... ['queen', 'king'] word_list[0] queen word_list[1]  king\n",
      "......i..... 37 ....word_list.... ['queen', 'rule'] word_list[0] queen word_list[1]  rule\n",
      "......i..... 38 ....word_list.... ['queen', 'realm'] word_list[0] queen word_list[1]  realm\n",
      "......i..... 39 ....word_list.... ['king', 'rule'] word_list[0] king word_list[1]  rule\n",
      "......i..... 40 ....word_list.... ['king', 'queen'] word_list[0] king word_list[1]  queen\n",
      "......i..... 41 ....word_list.... ['king', 'realm'] word_list[0] king word_list[1]  realm\n",
      "......i..... 42 ....word_list.... ['rule', 'realm'] word_list[0] rule word_list[1]  realm\n",
      "......i..... 43 ....word_list.... ['rule', 'king'] word_list[0] rule word_list[1]  king\n",
      "......i..... 44 ....word_list.... ['rule', 'queen'] word_list[0] rule word_list[1]  queen\n",
      "......i..... 45 ....word_list.... ['realm', 'rule'] word_list[0] realm word_list[1]  rule\n",
      "......i..... 46 ....word_list.... ['realm', 'king'] word_list[0] realm word_list[1]  king\n",
      "......i..... 47 ....word_list.... ['realm', 'queen'] word_list[0] realm word_list[1]  queen\n",
      "......i..... 48 ....word_list.... ['prince', 'strong'] word_list[0] prince word_list[1]  strong\n",
      "......i..... 49 ....word_list.... ['prince', 'man'] word_list[0] prince word_list[1]  man\n",
      "......i..... 50 ....word_list.... ['strong', 'man'] word_list[0] strong word_list[1]  man\n",
      "......i..... 51 ....word_list.... ['strong', 'prince'] word_list[0] strong word_list[1]  prince\n",
      "......i..... 52 ....word_list.... ['man', 'strong'] word_list[0] man word_list[1]  strong\n",
      "......i..... 53 ....word_list.... ['man', 'prince'] word_list[0] man word_list[1]  prince\n",
      "......i..... 54 ....word_list.... ['princess', 'beautiful'] word_list[0] princess word_list[1]  beautiful\n",
      "......i..... 55 ....word_list.... ['princess', 'woman'] word_list[0] princess word_list[1]  woman\n",
      "......i..... 56 ....word_list.... ['beautiful', 'woman'] word_list[0] beautiful word_list[1]  woman\n",
      "......i..... 57 ....word_list.... ['beautiful', 'princess'] word_list[0] beautiful word_list[1]  princess\n",
      "......i..... 58 ....word_list.... ['woman', 'beautiful'] word_list[0] woman word_list[1]  beautiful\n",
      "......i..... 59 ....word_list.... ['woman', 'princess'] word_list[0] woman word_list[1]  princess\n",
      "......i..... 60 ....word_list.... ['royal', 'family'] word_list[0] royal word_list[1]  family\n",
      "......i..... 61 ....word_list.... ['royal', 'king'] word_list[0] royal word_list[1]  king\n",
      "......i..... 62 ....word_list.... ['royal', 'queen'] word_list[0] royal word_list[1]  queen\n",
      "......i..... 63 ....word_list.... ['family', 'king'] word_list[0] family word_list[1]  king\n",
      "......i..... 64 ....word_list.... ['family', 'royal'] word_list[0] family word_list[1]  royal\n",
      "......i..... 65 ....word_list.... ['family', 'queen'] word_list[0] family word_list[1]  queen\n",
      "......i..... 66 ....word_list.... ['family', 'their'] word_list[0] family word_list[1]  their\n",
      "......i..... 67 ....word_list.... ['king', 'queen'] word_list[0] king word_list[1]  queen\n",
      "......i..... 68 ....word_list.... ['king', 'family'] word_list[0] king word_list[1]  family\n",
      "......i..... 69 ....word_list.... ['king', 'their'] word_list[0] king word_list[1]  their\n",
      "......i..... 70 ....word_list.... ['king', 'royal'] word_list[0] king word_list[1]  royal\n",
      "......i..... 71 ....word_list.... ['king', 'children'] word_list[0] king word_list[1]  children\n",
      "......i..... 72 ....word_list.... ['queen', 'their'] word_list[0] queen word_list[1]  their\n",
      "......i..... 73 ....word_list.... ['queen', 'king'] word_list[0] queen word_list[1]  king\n",
      "......i..... 74 ....word_list.... ['queen', 'children'] word_list[0] queen word_list[1]  children\n",
      "......i..... 75 ....word_list.... ['queen', 'family'] word_list[0] queen word_list[1]  family\n",
      "......i..... 76 ....word_list.... ['queen', 'royal'] word_list[0] queen word_list[1]  royal\n",
      "......i..... 77 ....word_list.... ['their', 'children'] word_list[0] their word_list[1]  children\n",
      "......i..... 78 ....word_list.... ['their', 'queen'] word_list[0] their word_list[1]  queen\n",
      "......i..... 79 ....word_list.... ['their', 'king'] word_list[0] their word_list[1]  king\n",
      "......i..... 80 ....word_list.... ['their', 'family'] word_list[0] their word_list[1]  family\n",
      "......i..... 81 ....word_list.... ['children', 'their'] word_list[0] children word_list[1]  their\n",
      "......i..... 82 ....word_list.... ['children', 'queen'] word_list[0] children word_list[1]  queen\n",
      "......i..... 83 ....word_list.... ['children', 'king'] word_list[0] children word_list[1]  king\n",
      "......i..... 84 ....word_list.... ['prince', 'only'] word_list[0] prince word_list[1]  only\n",
      "......i..... 85 ....word_list.... ['prince', 'boy'] word_list[0] prince word_list[1]  boy\n",
      "......i..... 86 ....word_list.... ['prince', 'now'] word_list[0] prince word_list[1]  now\n",
      "......i..... 87 ....word_list.... ['only', 'boy'] word_list[0] only word_list[1]  boy\n",
      "......i..... 88 ....word_list.... ['only', 'prince'] word_list[0] only word_list[1]  prince\n",
      "......i..... 89 ....word_list.... ['only', 'now'] word_list[0] only word_list[1]  now\n",
      "......i..... 90 ....word_list.... ['boy', 'now'] word_list[0] boy word_list[1]  now\n",
      "......i..... 91 ....word_list.... ['boy', 'only'] word_list[0] boy word_list[1]  only\n",
      "......i..... 92 ....word_list.... ['boy', 'prince'] word_list[0] boy word_list[1]  prince\n",
      "......i..... 93 ....word_list.... ['now', 'boy'] word_list[0] now word_list[1]  boy\n",
      "......i..... 94 ....word_list.... ['now', 'only'] word_list[0] now word_list[1]  only\n",
      "......i..... 95 ....word_list.... ['now', 'prince'] word_list[0] now word_list[1]  prince\n",
      "......i..... 96 ....word_list.... ['boy', 'man'] word_list[0] boy word_list[1]  man\n",
      "......i..... 97 ....word_list.... ['man', 'boy'] word_list[0] man word_list[1]  boy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "98/98 [==============================] - 0s 429us/step - loss: 3.0668\n",
      "Epoch 2/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0659\n",
      "Epoch 3/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0649\n",
      "Epoch 4/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0640\n",
      "Epoch 5/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0630\n",
      "Epoch 6/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 3.0621\n",
      "Epoch 7/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0611\n",
      "Epoch 8/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0602\n",
      "Epoch 9/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0593\n",
      "Epoch 10/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0583\n",
      "Epoch 11/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0574\n",
      "Epoch 12/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0565\n",
      "Epoch 13/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0556\n",
      "Epoch 14/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0546\n",
      "Epoch 15/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0537\n",
      "Epoch 16/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0528\n",
      "Epoch 17/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0519\n",
      "Epoch 18/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0510\n",
      "Epoch 19/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0501\n",
      "Epoch 20/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0492\n",
      "Epoch 21/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 3.0483\n",
      "Epoch 22/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0474\n",
      "Epoch 23/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0465\n",
      "Epoch 24/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 3.0456\n",
      "Epoch 25/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0447\n",
      "Epoch 26/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0438\n",
      "Epoch 27/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0429\n",
      "Epoch 28/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0420\n",
      "Epoch 29/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0411\n",
      "Epoch 30/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0403\n",
      "Epoch 31/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0394\n",
      "Epoch 32/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 3.0385\n",
      "Epoch 33/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0376\n",
      "Epoch 34/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0368\n",
      "Epoch 35/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0359\n",
      "Epoch 36/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0351\n",
      "Epoch 37/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0342\n",
      "Epoch 38/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0334\n",
      "Epoch 39/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0325\n",
      "Epoch 40/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0317\n",
      "Epoch 41/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0308\n",
      "Epoch 42/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0300\n",
      "Epoch 43/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0291\n",
      "Epoch 44/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0283\n",
      "Epoch 45/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0274\n",
      "Epoch 46/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 3.0266\n",
      "Epoch 47/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0258\n",
      "Epoch 48/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0249\n",
      "Epoch 49/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0241\n",
      "Epoch 50/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0233\n",
      "Epoch 51/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0224\n",
      "Epoch 52/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0216\n",
      "Epoch 53/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0208\n",
      "Epoch 54/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0200\n",
      "Epoch 55/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0192\n",
      "Epoch 56/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0183\n",
      "Epoch 57/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0175\n",
      "Epoch 58/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 3.0167\n",
      "Epoch 59/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0159\n",
      "Epoch 60/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0151\n",
      "Epoch 61/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 3.0143\n",
      "Epoch 62/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0135\n",
      "Epoch 63/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0126\n",
      "Epoch 64/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0118\n",
      "Epoch 65/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0110\n",
      "Epoch 66/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0102\n",
      "Epoch 67/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0094\n",
      "Epoch 68/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0086\n",
      "Epoch 69/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 3.0078\n",
      "Epoch 70/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0070\n",
      "Epoch 71/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0062\n",
      "Epoch 72/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0054\n",
      "Epoch 73/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 3.0046\n",
      "Epoch 74/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0038\n",
      "Epoch 75/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0030\n",
      "Epoch 76/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0022\n",
      "Epoch 77/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 3.0014\n",
      "Epoch 78/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 3.0006\n",
      "Epoch 79/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9998\n",
      "Epoch 80/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9991\n",
      "Epoch 81/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9983\n",
      "Epoch 82/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9975\n",
      "Epoch 83/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9967\n",
      "Epoch 84/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9959\n",
      "Epoch 85/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.9951\n",
      "Epoch 86/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9943\n",
      "Epoch 87/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9935\n",
      "Epoch 88/1000\n",
      "98/98 [==============================] - 0s 8us/step - loss: 2.9927\n",
      "Epoch 89/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.9919\n",
      "Epoch 90/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9911\n",
      "Epoch 91/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9903\n",
      "Epoch 92/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9895\n",
      "Epoch 93/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9888\n",
      "Epoch 94/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9880\n",
      "Epoch 95/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9872\n",
      "Epoch 96/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9864\n",
      "Epoch 97/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.9856\n",
      "Epoch 98/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9848\n",
      "Epoch 99/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9832\n",
      "Epoch 101/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9824\n",
      "Epoch 102/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9816\n",
      "Epoch 103/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9808\n",
      "Epoch 104/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9800\n",
      "Epoch 105/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9793\n",
      "Epoch 106/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9785\n",
      "Epoch 107/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9777\n",
      "Epoch 108/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9769\n",
      "Epoch 109/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9761\n",
      "Epoch 110/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9753\n",
      "Epoch 111/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9745\n",
      "Epoch 112/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9737\n",
      "Epoch 113/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9729\n",
      "Epoch 114/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9721\n",
      "Epoch 115/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9713\n",
      "Epoch 116/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9705\n",
      "Epoch 117/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9697\n",
      "Epoch 118/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9689\n",
      "Epoch 119/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9681\n",
      "Epoch 120/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.9673\n",
      "Epoch 121/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9665\n",
      "Epoch 122/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9657\n",
      "Epoch 123/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9649\n",
      "Epoch 124/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9641\n",
      "Epoch 125/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9633\n",
      "Epoch 126/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9625\n",
      "Epoch 127/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9617\n",
      "Epoch 128/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9609\n",
      "Epoch 129/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9601\n",
      "Epoch 130/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9593\n",
      "Epoch 131/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9585\n",
      "Epoch 132/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9577\n",
      "Epoch 133/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9569\n",
      "Epoch 134/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9561\n",
      "Epoch 135/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9553\n",
      "Epoch 136/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9545\n",
      "Epoch 137/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9537\n",
      "Epoch 138/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9529\n",
      "Epoch 139/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9521\n",
      "Epoch 140/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9512\n",
      "Epoch 141/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9504\n",
      "Epoch 142/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9496\n",
      "Epoch 143/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9488\n",
      "Epoch 144/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9480\n",
      "Epoch 145/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9472\n",
      "Epoch 146/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9464\n",
      "Epoch 147/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9456\n",
      "Epoch 148/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9448\n",
      "Epoch 149/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9439\n",
      "Epoch 150/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9431\n",
      "Epoch 151/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.9423\n",
      "Epoch 152/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9415\n",
      "Epoch 153/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9407\n",
      "Epoch 154/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9399\n",
      "Epoch 155/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9391\n",
      "Epoch 156/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9382\n",
      "Epoch 157/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9374\n",
      "Epoch 158/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9366\n",
      "Epoch 159/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9358\n",
      "Epoch 160/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9350\n",
      "Epoch 161/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9341\n",
      "Epoch 162/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9333\n",
      "Epoch 163/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9325\n",
      "Epoch 164/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9317\n",
      "Epoch 165/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9309\n",
      "Epoch 166/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9300\n",
      "Epoch 167/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9292\n",
      "Epoch 168/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9284\n",
      "Epoch 169/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9275\n",
      "Epoch 170/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9267\n",
      "Epoch 171/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9259\n",
      "Epoch 172/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9251\n",
      "Epoch 173/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9242\n",
      "Epoch 174/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9234\n",
      "Epoch 175/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9226\n",
      "Epoch 176/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9217\n",
      "Epoch 177/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9209\n",
      "Epoch 178/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9200\n",
      "Epoch 179/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.9192\n",
      "Epoch 180/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9184\n",
      "Epoch 181/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9175\n",
      "Epoch 182/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9167\n",
      "Epoch 183/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9158\n",
      "Epoch 184/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9150\n",
      "Epoch 185/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9141\n",
      "Epoch 186/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9133\n",
      "Epoch 187/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9125\n",
      "Epoch 188/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9116\n",
      "Epoch 189/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9108\n",
      "Epoch 190/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9099\n",
      "Epoch 191/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9091\n",
      "Epoch 192/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9082\n",
      "Epoch 193/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9073\n",
      "Epoch 194/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9065\n",
      "Epoch 195/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9056\n",
      "Epoch 196/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9048\n",
      "Epoch 197/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.9039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9031\n",
      "Epoch 199/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9022\n",
      "Epoch 200/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9013\n",
      "Epoch 201/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.9005\n",
      "Epoch 202/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8996\n",
      "Epoch 203/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8987\n",
      "Epoch 204/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8979\n",
      "Epoch 205/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8970\n",
      "Epoch 206/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8961\n",
      "Epoch 207/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8953\n",
      "Epoch 208/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.8944\n",
      "Epoch 209/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8935\n",
      "Epoch 210/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8926\n",
      "Epoch 211/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8918\n",
      "Epoch 212/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8909\n",
      "Epoch 213/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8900\n",
      "Epoch 214/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8891\n",
      "Epoch 215/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8882\n",
      "Epoch 216/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.8874\n",
      "Epoch 217/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8865\n",
      "Epoch 218/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8856\n",
      "Epoch 219/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8847\n",
      "Epoch 220/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8838\n",
      "Epoch 221/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8829\n",
      "Epoch 222/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8820\n",
      "Epoch 223/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8812\n",
      "Epoch 224/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8803\n",
      "Epoch 225/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8794\n",
      "Epoch 226/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8785\n",
      "Epoch 227/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8776\n",
      "Epoch 228/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8767\n",
      "Epoch 229/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8758\n",
      "Epoch 230/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8749\n",
      "Epoch 231/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8740\n",
      "Epoch 232/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8731\n",
      "Epoch 233/1000\n",
      "98/98 [==============================] - 0s 11us/step - loss: 2.8722\n",
      "Epoch 234/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8713\n",
      "Epoch 235/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8704\n",
      "Epoch 236/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8695\n",
      "Epoch 237/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8686\n",
      "Epoch 238/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.8677\n",
      "Epoch 239/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8668\n",
      "Epoch 240/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8659\n",
      "Epoch 241/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8650\n",
      "Epoch 242/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.8640\n",
      "Epoch 243/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8631\n",
      "Epoch 244/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8622\n",
      "Epoch 245/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.8613\n",
      "Epoch 246/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8604\n",
      "Epoch 247/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.8595\n",
      "Epoch 248/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8586\n",
      "Epoch 249/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8577\n",
      "Epoch 250/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8567\n",
      "Epoch 251/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.8558\n",
      "Epoch 252/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8549\n",
      "Epoch 253/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.8540\n",
      "Epoch 254/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8531\n",
      "Epoch 255/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8521\n",
      "Epoch 256/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.8512\n",
      "Epoch 257/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8503\n",
      "Epoch 258/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.8494\n",
      "Epoch 259/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8484\n",
      "Epoch 260/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.8475\n",
      "Epoch 261/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.8466\n",
      "Epoch 262/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8457\n",
      "Epoch 263/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8447\n",
      "Epoch 264/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8438\n",
      "Epoch 265/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8429\n",
      "Epoch 266/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8419\n",
      "Epoch 267/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8410\n",
      "Epoch 268/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8401\n",
      "Epoch 269/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8391\n",
      "Epoch 270/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8382\n",
      "Epoch 271/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8373\n",
      "Epoch 272/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8363\n",
      "Epoch 273/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8354\n",
      "Epoch 274/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.8344\n",
      "Epoch 275/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.8335\n",
      "Epoch 276/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8326\n",
      "Epoch 277/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8316\n",
      "Epoch 278/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8307\n",
      "Epoch 279/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8297\n",
      "Epoch 280/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8288\n",
      "Epoch 281/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8279\n",
      "Epoch 282/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8269\n",
      "Epoch 283/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8260\n",
      "Epoch 284/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8250\n",
      "Epoch 285/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8241\n",
      "Epoch 286/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8231\n",
      "Epoch 287/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8222\n",
      "Epoch 288/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8212\n",
      "Epoch 289/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8203\n",
      "Epoch 290/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8193\n",
      "Epoch 291/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8184\n",
      "Epoch 292/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8174\n",
      "Epoch 293/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.8165\n",
      "Epoch 294/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8155\n",
      "Epoch 295/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8136\n",
      "Epoch 297/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8126\n",
      "Epoch 298/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8117\n",
      "Epoch 299/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8107\n",
      "Epoch 300/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.8098\n",
      "Epoch 301/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.8088\n",
      "Epoch 302/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8079\n",
      "Epoch 303/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8069\n",
      "Epoch 304/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8059\n",
      "Epoch 305/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8050\n",
      "Epoch 306/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8040\n",
      "Epoch 307/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8031\n",
      "Epoch 308/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8021\n",
      "Epoch 309/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8011\n",
      "Epoch 310/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.8002\n",
      "Epoch 311/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7992\n",
      "Epoch 312/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7982\n",
      "Epoch 313/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7973\n",
      "Epoch 314/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7963\n",
      "Epoch 315/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.7953\n",
      "Epoch 316/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7944\n",
      "Epoch 317/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.7934\n",
      "Epoch 318/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7924\n",
      "Epoch 319/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7915\n",
      "Epoch 320/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7905\n",
      "Epoch 321/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7895\n",
      "Epoch 322/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7885\n",
      "Epoch 323/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7876\n",
      "Epoch 324/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7866\n",
      "Epoch 325/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7856\n",
      "Epoch 326/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.7846\n",
      "Epoch 327/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7837\n",
      "Epoch 328/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7827\n",
      "Epoch 329/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7817\n",
      "Epoch 330/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7808\n",
      "Epoch 331/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7798\n",
      "Epoch 332/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7788\n",
      "Epoch 333/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7778\n",
      "Epoch 334/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7768\n",
      "Epoch 335/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7759\n",
      "Epoch 336/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7749\n",
      "Epoch 337/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7739\n",
      "Epoch 338/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7729\n",
      "Epoch 339/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7720\n",
      "Epoch 340/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7710\n",
      "Epoch 341/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.7700\n",
      "Epoch 342/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.7690\n",
      "Epoch 343/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.7680\n",
      "Epoch 344/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.7670\n",
      "Epoch 345/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7661\n",
      "Epoch 346/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7651\n",
      "Epoch 347/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7641\n",
      "Epoch 348/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7631\n",
      "Epoch 349/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7621\n",
      "Epoch 350/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7612\n",
      "Epoch 351/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7602\n",
      "Epoch 352/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7592\n",
      "Epoch 353/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7582\n",
      "Epoch 354/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7572\n",
      "Epoch 355/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7562\n",
      "Epoch 356/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7553\n",
      "Epoch 357/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.7543\n",
      "Epoch 358/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7533\n",
      "Epoch 359/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.7523\n",
      "Epoch 360/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7513\n",
      "Epoch 361/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7503\n",
      "Epoch 362/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7493\n",
      "Epoch 363/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7484\n",
      "Epoch 364/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7474\n",
      "Epoch 365/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7464\n",
      "Epoch 366/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7454\n",
      "Epoch 367/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7444\n",
      "Epoch 368/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7434\n",
      "Epoch 369/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7424\n",
      "Epoch 370/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7415\n",
      "Epoch 371/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7405\n",
      "Epoch 372/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7395\n",
      "Epoch 373/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7385\n",
      "Epoch 374/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7375\n",
      "Epoch 375/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7365\n",
      "Epoch 376/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7355\n",
      "Epoch 377/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7346\n",
      "Epoch 378/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7336\n",
      "Epoch 379/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7326\n",
      "Epoch 380/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7316\n",
      "Epoch 381/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7306\n",
      "Epoch 382/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7296\n",
      "Epoch 383/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7287\n",
      "Epoch 384/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7277\n",
      "Epoch 385/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7267\n",
      "Epoch 386/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7257\n",
      "Epoch 387/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7247\n",
      "Epoch 388/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7237\n",
      "Epoch 389/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7228\n",
      "Epoch 390/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7218\n",
      "Epoch 391/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7208\n",
      "Epoch 392/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7198\n",
      "Epoch 393/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 394/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7179\n",
      "Epoch 395/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7169\n",
      "Epoch 396/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7159\n",
      "Epoch 397/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.7149\n",
      "Epoch 398/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7139\n",
      "Epoch 399/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7130\n",
      "Epoch 400/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.7120\n",
      "Epoch 401/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7110\n",
      "Epoch 402/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7100\n",
      "Epoch 403/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7091\n",
      "Epoch 404/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7081\n",
      "Epoch 405/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7071\n",
      "Epoch 406/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7061\n",
      "Epoch 407/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7052\n",
      "Epoch 408/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7042\n",
      "Epoch 409/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7032\n",
      "Epoch 410/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7023\n",
      "Epoch 411/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.7013\n",
      "Epoch 412/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.7003\n",
      "Epoch 413/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6993\n",
      "Epoch 414/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.6984\n",
      "Epoch 415/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6974\n",
      "Epoch 416/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6965\n",
      "Epoch 417/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6955\n",
      "Epoch 418/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6945\n",
      "Epoch 419/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6936\n",
      "Epoch 420/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6926\n",
      "Epoch 421/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6916\n",
      "Epoch 422/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6907\n",
      "Epoch 423/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6897\n",
      "Epoch 424/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6888\n",
      "Epoch 425/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6878\n",
      "Epoch 426/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6868\n",
      "Epoch 427/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6859\n",
      "Epoch 428/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6849\n",
      "Epoch 429/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6840\n",
      "Epoch 430/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6830\n",
      "Epoch 431/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6821\n",
      "Epoch 432/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6811\n",
      "Epoch 433/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6802\n",
      "Epoch 434/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6792\n",
      "Epoch 435/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6783\n",
      "Epoch 436/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6774\n",
      "Epoch 437/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6764\n",
      "Epoch 438/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6755\n",
      "Epoch 439/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6745\n",
      "Epoch 440/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6736\n",
      "Epoch 441/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6726\n",
      "Epoch 442/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6717\n",
      "Epoch 443/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6708\n",
      "Epoch 444/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6698\n",
      "Epoch 445/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6689\n",
      "Epoch 446/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6680\n",
      "Epoch 447/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6670\n",
      "Epoch 448/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6661\n",
      "Epoch 449/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6652\n",
      "Epoch 450/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6643\n",
      "Epoch 451/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6633\n",
      "Epoch 452/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6624\n",
      "Epoch 453/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6615\n",
      "Epoch 454/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6606\n",
      "Epoch 455/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6597\n",
      "Epoch 456/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6587\n",
      "Epoch 457/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6578\n",
      "Epoch 458/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6569\n",
      "Epoch 459/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6560\n",
      "Epoch 460/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6551\n",
      "Epoch 461/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6542\n",
      "Epoch 462/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6533\n",
      "Epoch 463/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6524\n",
      "Epoch 464/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6515\n",
      "Epoch 465/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6505\n",
      "Epoch 466/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6496\n",
      "Epoch 467/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6487\n",
      "Epoch 468/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6478\n",
      "Epoch 469/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6470\n",
      "Epoch 470/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6461\n",
      "Epoch 471/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6452\n",
      "Epoch 472/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6443\n",
      "Epoch 473/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6434\n",
      "Epoch 474/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6425\n",
      "Epoch 475/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6416\n",
      "Epoch 476/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6407\n",
      "Epoch 477/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6398\n",
      "Epoch 478/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6390\n",
      "Epoch 479/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6381\n",
      "Epoch 480/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6372\n",
      "Epoch 481/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6363\n",
      "Epoch 482/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6354\n",
      "Epoch 483/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6346\n",
      "Epoch 484/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6337\n",
      "Epoch 485/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6328\n",
      "Epoch 486/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6320\n",
      "Epoch 487/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6311\n",
      "Epoch 488/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6302\n",
      "Epoch 489/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6294\n",
      "Epoch 490/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6285\n",
      "Epoch 491/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 492/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6268\n",
      "Epoch 493/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6259\n",
      "Epoch 494/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6251\n",
      "Epoch 495/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6242\n",
      "Epoch 496/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6234\n",
      "Epoch 497/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6225\n",
      "Epoch 498/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6217\n",
      "Epoch 499/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6209\n",
      "Epoch 500/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6200\n",
      "Epoch 501/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6192\n",
      "Epoch 502/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6183\n",
      "Epoch 503/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.6175\n",
      "Epoch 504/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6167\n",
      "Epoch 505/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6158\n",
      "Epoch 506/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6150\n",
      "Epoch 507/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6142\n",
      "Epoch 508/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6133\n",
      "Epoch 509/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6125\n",
      "Epoch 510/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6117\n",
      "Epoch 511/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6109\n",
      "Epoch 512/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6100\n",
      "Epoch 513/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6092\n",
      "Epoch 514/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6084\n",
      "Epoch 515/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6076\n",
      "Epoch 516/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6068\n",
      "Epoch 517/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.6060\n",
      "Epoch 518/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6052\n",
      "Epoch 519/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6044\n",
      "Epoch 520/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6036\n",
      "Epoch 521/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6028\n",
      "Epoch 522/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6020\n",
      "Epoch 523/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.6012\n",
      "Epoch 524/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.6004\n",
      "Epoch 525/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5996\n",
      "Epoch 526/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5988\n",
      "Epoch 527/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5980\n",
      "Epoch 528/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5972\n",
      "Epoch 529/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5964\n",
      "Epoch 530/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5956\n",
      "Epoch 531/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5948\n",
      "Epoch 532/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5940\n",
      "Epoch 533/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5933\n",
      "Epoch 534/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5925\n",
      "Epoch 535/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5917\n",
      "Epoch 536/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5909\n",
      "Epoch 537/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5902\n",
      "Epoch 538/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5894\n",
      "Epoch 539/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5886\n",
      "Epoch 540/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5879\n",
      "Epoch 541/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5871\n",
      "Epoch 542/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5863\n",
      "Epoch 543/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5856\n",
      "Epoch 544/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5848\n",
      "Epoch 545/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5840\n",
      "Epoch 546/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5833\n",
      "Epoch 547/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5825\n",
      "Epoch 548/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5818\n",
      "Epoch 549/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5810\n",
      "Epoch 550/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5803\n",
      "Epoch 551/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5795\n",
      "Epoch 552/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.5788\n",
      "Epoch 553/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5780\n",
      "Epoch 554/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5773\n",
      "Epoch 555/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5766\n",
      "Epoch 556/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5758\n",
      "Epoch 557/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5751\n",
      "Epoch 558/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5744\n",
      "Epoch 559/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5736\n",
      "Epoch 560/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5729\n",
      "Epoch 561/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5722\n",
      "Epoch 562/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5714\n",
      "Epoch 563/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5707\n",
      "Epoch 564/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5700\n",
      "Epoch 565/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5693\n",
      "Epoch 566/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5685\n",
      "Epoch 567/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5678\n",
      "Epoch 568/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5671\n",
      "Epoch 569/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5664\n",
      "Epoch 570/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5657\n",
      "Epoch 571/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5650\n",
      "Epoch 572/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5643\n",
      "Epoch 573/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5635\n",
      "Epoch 574/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.5628\n",
      "Epoch 575/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5621\n",
      "Epoch 576/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5614\n",
      "Epoch 577/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5607\n",
      "Epoch 578/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5600\n",
      "Epoch 579/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5593\n",
      "Epoch 580/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5586\n",
      "Epoch 581/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5579\n",
      "Epoch 582/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5572\n",
      "Epoch 583/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5566\n",
      "Epoch 584/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5559\n",
      "Epoch 585/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5552\n",
      "Epoch 586/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5545\n",
      "Epoch 587/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5538\n",
      "Epoch 588/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5531\n",
      "Epoch 589/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5518\n",
      "Epoch 591/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5511\n",
      "Epoch 592/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5504\n",
      "Epoch 593/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5497\n",
      "Epoch 594/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5491\n",
      "Epoch 595/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5484\n",
      "Epoch 596/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5477\n",
      "Epoch 597/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5470\n",
      "Epoch 598/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5464\n",
      "Epoch 599/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5457\n",
      "Epoch 600/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5451\n",
      "Epoch 601/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5444\n",
      "Epoch 602/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5437\n",
      "Epoch 603/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5431\n",
      "Epoch 604/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5424\n",
      "Epoch 605/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5418\n",
      "Epoch 606/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5411\n",
      "Epoch 607/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5405\n",
      "Epoch 608/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5398\n",
      "Epoch 609/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5392\n",
      "Epoch 610/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5385\n",
      "Epoch 611/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5379\n",
      "Epoch 612/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5372\n",
      "Epoch 613/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5366\n",
      "Epoch 614/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5359\n",
      "Epoch 615/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5353\n",
      "Epoch 616/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5347\n",
      "Epoch 617/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5340\n",
      "Epoch 618/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5334\n",
      "Epoch 619/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5328\n",
      "Epoch 620/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5321\n",
      "Epoch 621/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5315\n",
      "Epoch 622/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5309\n",
      "Epoch 623/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5302\n",
      "Epoch 624/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5296\n",
      "Epoch 625/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5290\n",
      "Epoch 626/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5284\n",
      "Epoch 627/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5277\n",
      "Epoch 628/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5271\n",
      "Epoch 629/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5265\n",
      "Epoch 630/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5259\n",
      "Epoch 631/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5253\n",
      "Epoch 632/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5246\n",
      "Epoch 633/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5240\n",
      "Epoch 634/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5234\n",
      "Epoch 635/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5228\n",
      "Epoch 636/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5222\n",
      "Epoch 637/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5216\n",
      "Epoch 638/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5210\n",
      "Epoch 639/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5204\n",
      "Epoch 640/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5198\n",
      "Epoch 641/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5192\n",
      "Epoch 642/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5186\n",
      "Epoch 643/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5180\n",
      "Epoch 644/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5174\n",
      "Epoch 645/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5168\n",
      "Epoch 646/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5162\n",
      "Epoch 647/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5156\n",
      "Epoch 648/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5150\n",
      "Epoch 649/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5144\n",
      "Epoch 650/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5138\n",
      "Epoch 651/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5132\n",
      "Epoch 652/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5127\n",
      "Epoch 653/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5121\n",
      "Epoch 654/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5115\n",
      "Epoch 655/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5109\n",
      "Epoch 656/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5103\n",
      "Epoch 657/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5098\n",
      "Epoch 658/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5092\n",
      "Epoch 659/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5086\n",
      "Epoch 660/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5080\n",
      "Epoch 661/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5075\n",
      "Epoch 662/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5069\n",
      "Epoch 663/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5063\n",
      "Epoch 664/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5057\n",
      "Epoch 665/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5052\n",
      "Epoch 666/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5046\n",
      "Epoch 667/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.5040\n",
      "Epoch 668/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5035\n",
      "Epoch 669/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5029\n",
      "Epoch 670/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.5024\n",
      "Epoch 671/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5018\n",
      "Epoch 672/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5012\n",
      "Epoch 673/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5007\n",
      "Epoch 674/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.5001\n",
      "Epoch 675/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4996\n",
      "Epoch 676/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4990\n",
      "Epoch 677/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4985\n",
      "Epoch 678/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4979\n",
      "Epoch 679/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4974\n",
      "Epoch 680/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4968\n",
      "Epoch 681/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4963\n",
      "Epoch 682/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4957\n",
      "Epoch 683/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4952\n",
      "Epoch 684/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4946\n",
      "Epoch 685/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4941\n",
      "Epoch 686/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4935\n",
      "Epoch 687/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 688/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4925\n",
      "Epoch 689/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4919\n",
      "Epoch 690/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4914\n",
      "Epoch 691/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4909\n",
      "Epoch 692/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4903\n",
      "Epoch 693/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4898\n",
      "Epoch 694/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4893\n",
      "Epoch 695/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4887\n",
      "Epoch 696/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4882\n",
      "Epoch 697/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4877\n",
      "Epoch 698/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4872\n",
      "Epoch 699/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4866\n",
      "Epoch 700/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.4861\n",
      "Epoch 701/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4856\n",
      "Epoch 702/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4851\n",
      "Epoch 703/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4846\n",
      "Epoch 704/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4840\n",
      "Epoch 705/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4835\n",
      "Epoch 706/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4830\n",
      "Epoch 707/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4825\n",
      "Epoch 708/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4820\n",
      "Epoch 709/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4815\n",
      "Epoch 710/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.4810\n",
      "Epoch 711/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4805\n",
      "Epoch 712/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4799\n",
      "Epoch 713/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4794\n",
      "Epoch 714/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4789\n",
      "Epoch 715/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4784\n",
      "Epoch 716/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4779\n",
      "Epoch 717/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4774\n",
      "Epoch 718/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4769\n",
      "Epoch 719/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4764\n",
      "Epoch 720/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4759\n",
      "Epoch 721/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4754\n",
      "Epoch 722/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4749\n",
      "Epoch 723/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4744\n",
      "Epoch 724/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4740\n",
      "Epoch 725/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4735\n",
      "Epoch 726/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4730\n",
      "Epoch 727/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4725\n",
      "Epoch 728/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4720\n",
      "Epoch 729/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4715\n",
      "Epoch 730/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4710\n",
      "Epoch 731/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4705\n",
      "Epoch 732/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4701\n",
      "Epoch 733/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4696\n",
      "Epoch 734/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4691\n",
      "Epoch 735/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4686\n",
      "Epoch 736/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4681\n",
      "Epoch 737/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4677\n",
      "Epoch 738/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4672\n",
      "Epoch 739/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4667\n",
      "Epoch 740/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4662\n",
      "Epoch 741/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4658\n",
      "Epoch 742/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4653\n",
      "Epoch 743/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4648\n",
      "Epoch 744/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4643\n",
      "Epoch 745/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4639\n",
      "Epoch 746/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4634\n",
      "Epoch 747/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4629\n",
      "Epoch 748/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4625\n",
      "Epoch 749/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.4620\n",
      "Epoch 750/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4616\n",
      "Epoch 751/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4611\n",
      "Epoch 752/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4606\n",
      "Epoch 753/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.4602\n",
      "Epoch 754/1000\n",
      "98/98 [==============================] - 0s 11us/step - loss: 2.4597\n",
      "Epoch 755/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4593\n",
      "Epoch 756/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4588\n",
      "Epoch 757/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4584\n",
      "Epoch 758/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4579\n",
      "Epoch 759/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4575\n",
      "Epoch 760/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4570\n",
      "Epoch 761/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4566\n",
      "Epoch 762/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4561\n",
      "Epoch 763/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4557\n",
      "Epoch 764/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4552\n",
      "Epoch 765/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4548\n",
      "Epoch 766/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4543\n",
      "Epoch 767/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4539\n",
      "Epoch 768/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4534\n",
      "Epoch 769/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4530\n",
      "Epoch 770/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4526\n",
      "Epoch 771/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4521\n",
      "Epoch 772/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4517\n",
      "Epoch 773/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4512\n",
      "Epoch 774/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4508\n",
      "Epoch 775/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4504\n",
      "Epoch 776/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4499\n",
      "Epoch 777/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4495\n",
      "Epoch 778/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4491\n",
      "Epoch 779/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4487\n",
      "Epoch 780/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4482\n",
      "Epoch 781/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4478\n",
      "Epoch 782/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4474\n",
      "Epoch 783/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4469\n",
      "Epoch 784/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4465\n",
      "Epoch 785/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 786/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4457\n",
      "Epoch 787/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4453\n",
      "Epoch 788/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4448\n",
      "Epoch 789/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4444\n",
      "Epoch 790/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4440\n",
      "Epoch 791/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4436\n",
      "Epoch 792/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4432\n",
      "Epoch 793/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4428\n",
      "Epoch 794/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4423\n",
      "Epoch 795/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4419\n",
      "Epoch 796/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4415\n",
      "Epoch 797/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4411\n",
      "Epoch 798/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4407\n",
      "Epoch 799/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4403\n",
      "Epoch 800/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4399\n",
      "Epoch 801/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4395\n",
      "Epoch 802/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4391\n",
      "Epoch 803/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4387\n",
      "Epoch 804/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4383\n",
      "Epoch 805/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4379\n",
      "Epoch 806/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4375\n",
      "Epoch 807/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4371\n",
      "Epoch 808/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4367\n",
      "Epoch 809/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4363\n",
      "Epoch 810/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4359\n",
      "Epoch 811/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4355\n",
      "Epoch 812/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4351\n",
      "Epoch 813/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4347\n",
      "Epoch 814/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4343\n",
      "Epoch 815/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4339\n",
      "Epoch 816/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.4335\n",
      "Epoch 817/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4331\n",
      "Epoch 818/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4327\n",
      "Epoch 819/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4324\n",
      "Epoch 820/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4320\n",
      "Epoch 821/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4316\n",
      "Epoch 822/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4312\n",
      "Epoch 823/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4308\n",
      "Epoch 824/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4304\n",
      "Epoch 825/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4301\n",
      "Epoch 826/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4297\n",
      "Epoch 827/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4293\n",
      "Epoch 828/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4289\n",
      "Epoch 829/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4285\n",
      "Epoch 830/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4282\n",
      "Epoch 831/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4278\n",
      "Epoch 832/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4274\n",
      "Epoch 833/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4270\n",
      "Epoch 834/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4267\n",
      "Epoch 835/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4263\n",
      "Epoch 836/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4259\n",
      "Epoch 837/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4256\n",
      "Epoch 838/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4252\n",
      "Epoch 839/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4248\n",
      "Epoch 840/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4245\n",
      "Epoch 841/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4241\n",
      "Epoch 842/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4237\n",
      "Epoch 843/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4234\n",
      "Epoch 844/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4230\n",
      "Epoch 845/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4226\n",
      "Epoch 846/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4223\n",
      "Epoch 847/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4219\n",
      "Epoch 848/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4216\n",
      "Epoch 849/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4212\n",
      "Epoch 850/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4209\n",
      "Epoch 851/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4205\n",
      "Epoch 852/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4201\n",
      "Epoch 853/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4198\n",
      "Epoch 854/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4194\n",
      "Epoch 855/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4191\n",
      "Epoch 856/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4187\n",
      "Epoch 857/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4184\n",
      "Epoch 858/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4180\n",
      "Epoch 859/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4177\n",
      "Epoch 860/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4173\n",
      "Epoch 861/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4170\n",
      "Epoch 862/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4166\n",
      "Epoch 863/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.4163\n",
      "Epoch 864/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4160\n",
      "Epoch 865/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4156\n",
      "Epoch 866/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4153\n",
      "Epoch 867/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4149\n",
      "Epoch 868/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4146\n",
      "Epoch 869/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4143\n",
      "Epoch 870/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4139\n",
      "Epoch 871/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4136\n",
      "Epoch 872/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4132\n",
      "Epoch 873/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4129\n",
      "Epoch 874/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4126\n",
      "Epoch 875/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4122\n",
      "Epoch 876/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.4119\n",
      "Epoch 877/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4116\n",
      "Epoch 878/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4112\n",
      "Epoch 879/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4109\n",
      "Epoch 880/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4106\n",
      "Epoch 881/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4103\n",
      "Epoch 882/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4099\n",
      "Epoch 883/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 884/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4093\n",
      "Epoch 885/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4090\n",
      "Epoch 886/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4086\n",
      "Epoch 887/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4083\n",
      "Epoch 888/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4080\n",
      "Epoch 889/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4077\n",
      "Epoch 890/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4073\n",
      "Epoch 891/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4070\n",
      "Epoch 892/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4067\n",
      "Epoch 893/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4064\n",
      "Epoch 894/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4061\n",
      "Epoch 895/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4057\n",
      "Epoch 896/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4054\n",
      "Epoch 897/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4051\n",
      "Epoch 898/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4048\n",
      "Epoch 899/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4045\n",
      "Epoch 900/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4042\n",
      "Epoch 901/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4039\n",
      "Epoch 902/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4036\n",
      "Epoch 903/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4032\n",
      "Epoch 904/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.4029\n",
      "Epoch 905/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4026\n",
      "Epoch 906/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4023\n",
      "Epoch 907/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4020\n",
      "Epoch 908/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4017\n",
      "Epoch 909/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4014\n",
      "Epoch 910/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4011\n",
      "Epoch 911/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4008\n",
      "Epoch 912/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4005\n",
      "Epoch 913/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.4002\n",
      "Epoch 914/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.3999\n",
      "Epoch 915/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3996\n",
      "Epoch 916/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3993\n",
      "Epoch 917/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3990\n",
      "Epoch 918/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3987\n",
      "Epoch 919/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3984\n",
      "Epoch 920/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3981\n",
      "Epoch 921/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3978\n",
      "Epoch 922/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3975\n",
      "Epoch 923/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3972\n",
      "Epoch 924/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3969\n",
      "Epoch 925/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3966\n",
      "Epoch 926/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3963\n",
      "Epoch 927/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.3960\n",
      "Epoch 928/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3958\n",
      "Epoch 929/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.3955\n",
      "Epoch 930/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3952\n",
      "Epoch 931/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.3949\n",
      "Epoch 932/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3946\n",
      "Epoch 933/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3943\n",
      "Epoch 934/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.3940\n",
      "Epoch 935/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3937\n",
      "Epoch 936/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.3935\n",
      "Epoch 937/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3932\n",
      "Epoch 938/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3929\n",
      "Epoch 939/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3926\n",
      "Epoch 940/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3923\n",
      "Epoch 941/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3921\n",
      "Epoch 942/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3918\n",
      "Epoch 943/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3915\n",
      "Epoch 944/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3912\n",
      "Epoch 945/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.3909\n",
      "Epoch 946/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3907\n",
      "Epoch 947/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3904\n",
      "Epoch 948/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3901\n",
      "Epoch 949/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3898\n",
      "Epoch 950/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.3896\n",
      "Epoch 951/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3893\n",
      "Epoch 952/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3890\n",
      "Epoch 953/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3887\n",
      "Epoch 954/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3885\n",
      "Epoch 955/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3882\n",
      "Epoch 956/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.3879\n",
      "Epoch 957/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3877\n",
      "Epoch 958/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3874\n",
      "Epoch 959/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3871\n",
      "Epoch 960/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3869\n",
      "Epoch 961/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3866\n",
      "Epoch 962/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3863\n",
      "Epoch 963/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3861\n",
      "Epoch 964/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3858\n",
      "Epoch 965/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3855\n",
      "Epoch 966/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3853\n",
      "Epoch 967/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3850\n",
      "Epoch 968/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3848\n",
      "Epoch 969/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3845\n",
      "Epoch 970/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3842\n",
      "Epoch 971/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3840\n",
      "Epoch 972/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3837\n",
      "Epoch 973/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.3835\n",
      "Epoch 974/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.3832\n",
      "Epoch 975/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.3829\n",
      "Epoch 976/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3827\n",
      "Epoch 977/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.3824\n",
      "Epoch 978/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3822\n",
      "Epoch 979/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3819\n",
      "Epoch 980/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3817\n",
      "Epoch 981/1000\n",
      "98/98 [==============================] - 0s 18us/step - loss: 2.3814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 982/1000\n",
      "98/98 [==============================] - 0s 20us/step - loss: 2.3812\n",
      "Epoch 983/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3809\n",
      "Epoch 984/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3807\n",
      "Epoch 985/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3804\n",
      "Epoch 986/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3802\n",
      "Epoch 987/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3799\n",
      "Epoch 988/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3797\n",
      "Epoch 989/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3794\n",
      "Epoch 990/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3792\n",
      "Epoch 991/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3789\n",
      "Epoch 992/1000\n",
      "98/98 [==============================] - 0s 0us/step - loss: 2.3787\n",
      "Epoch 993/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3784\n",
      "Epoch 994/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3782\n",
      "Epoch 995/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3779\n",
      "Epoch 996/1000\n",
      "98/98 [==============================] - 0s 11us/step - loss: 2.3777\n",
      "Epoch 997/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3774\n",
      "Epoch 998/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3772\n",
      "Epoch 999/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3770\n",
      "Epoch 1000/1000\n",
      "98/98 [==============================] - 0s 10us/step - loss: 2.3767\n",
      "Cannot create output folder: [WinError 183] Cannot create a file when that file already exists: 'E:\\\\one neuron\\\\NLP\\\\word-embedding-creation-master\\\\output'\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Drawing the embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep learning: \n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# Custom functions\n",
    "#from utility import text_preprocessing, create_unique_word_dict\n",
    "\n",
    "# Reading the text from the input folder\n",
    "texts = pd.read_csv('input/sample.csv')\n",
    "texts = [x for x in texts['text']]\n",
    "\n",
    "# Defining the window for context\n",
    "window = 2\n",
    "\n",
    "# Creating a placeholder for the scanning of the word list\n",
    "word_lists = []\n",
    "all_text = []\n",
    "\n",
    "for text in texts:\n",
    "\n",
    "    # Cleaning the text\n",
    "    text = text_preprocessing(text)\n",
    "    print(text)\n",
    "    # Appending to the all text list\n",
    "    all_text += text \n",
    "\n",
    "    # Creating a context dictionary\n",
    "    for i, word in enumerate(text):\n",
    "        print(\"......i.....\" ,i,\"....word....\",word)\n",
    "        for w in range(3):\n",
    "            # Getting the context that is ahead by *window* words\n",
    "            if i + 1 + w < len(text): \n",
    "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "            # Getting the context that is behind by *window* words    \n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists.append([word] + [text[(i - w - 1)]])\n",
    "\n",
    "                \n",
    "print(\"word list \" , word_lists)\n",
    "unique_word_dict = create_unique_word_dict(all_text)\n",
    "print(\"unique_word_dict\",unique_word_dict)\n",
    "\n",
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "\n",
    "# Getting all the unique words \n",
    "words = list(unique_word_dict.keys())\n",
    "\n",
    "print(\"words\",words)\n",
    "\n",
    "# Creating the X and Y matrices using one hot encoding\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i, word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    \n",
    "    main_word_index = unique_word_dict.get(word_list[0])\n",
    "    context_word_index = unique_word_dict.get(word_list[1])\n",
    "    print(\"......i.....\" ,i,\"....word_list....\",word_list , \"word_list[0]\",word_list[0],\"word_list[1] \",word_list[1])\n",
    "    # Creating the placeholders   \n",
    "    X_row = np.zeros(n_words)\n",
    "    Y_row = np.zeros(n_words)\n",
    "\n",
    "    # One hot encoding the main word\n",
    "    X_row[main_word_index] = 1\n",
    "\n",
    "    # One hot encoding the Y matrix words \n",
    "    Y_row[context_word_index] = 1\n",
    "\n",
    "    # Appending to the main matrices\n",
    "    X.append(X_row)\n",
    "    Y.append(Y_row)\n",
    "\n",
    "# Converting the matrices into a sparse format because the vast majority of the data are 0s\n",
    "X = sparse.csr_matrix(X)\n",
    "Y = sparse.csr_matrix(Y)\n",
    "\n",
    "# Defining the size of the embedding\n",
    "embed_size = 2\n",
    "\n",
    "# Defining the neural network\n",
    "inp = Input(shape=(X.shape[1],))\n",
    "x = Dense(units=embed_size, activation='linear')(inp)\n",
    "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# Optimizing the network weights\n",
    "model.fit(\n",
    "    x=X, \n",
    "    y=Y, \n",
    "    batch_size=256,\n",
    "    epochs=1000\n",
    "    )\n",
    "\n",
    "# Obtaining the weights from the neural network. \n",
    "# These are the so called word embeddings\n",
    "\n",
    "# The input layer \n",
    "weights = model.get_weights()[0]\n",
    "\n",
    "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
    "# the value is the numeric vector\n",
    "embedding_dict = {}\n",
    "for word in words: \n",
    "    embedding_dict.update({\n",
    "        word: weights[unique_word_dict.get(word)]\n",
    "        })\n",
    "\n",
    "# Ploting the embeddings\n",
    "plt.figure(figsize=(10, 10))\n",
    "for word in list(unique_word_dict.keys()):\n",
    "    coord = embedding_dict.get(word)\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.annotate(word, (coord[0], coord[1]))       \n",
    "\n",
    "# Saving the embedding vector to a txt file\n",
    "try:\n",
    "    os.mkdir(f'{os.getcwd()}\\\\output')        \n",
    "except Exception as e:\n",
    "    print(f'Cannot create output folder: {e}')\n",
    "\n",
    "with open(f'{os.getcwd()}\\\\output\\\\embedding.txt', 'w') as f:\n",
    "    for key, value in embedding_dict.items():\n",
    "        try:\n",
    "            f.write(f'{key}: {value}\\n')   \n",
    "        except Exception as e:\n",
    "            print(f'Cannot write word {key} to dict: {e}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beautiful': array([-1.3688022, -1.2648398], dtype=float32),\n",
       " 'boy': array([-1.2045977,  1.3686044], dtype=float32),\n",
       " 'can': array([-0.5418091 ,  0.26029423], dtype=float32),\n",
       " 'children': array([ 0.6832872, -1.0086254], dtype=float32),\n",
       " 'daughter': array([-1.1178044, -1.1369839], dtype=float32),\n",
       " 'family': array([ 0.9929922, -0.9225423], dtype=float32),\n",
       " 'future': array([-0.43754777,  0.3495058 ], dtype=float32),\n",
       " 'king': array([0.8600612 , 0.08342463], dtype=float32),\n",
       " 'man': array([0.05348093, 1.1474305 ], dtype=float32),\n",
       " 'now': array([-0.94651306,  0.7657763 ], dtype=float32),\n",
       " 'only': array([-0.8729304 ,  0.46015522], dtype=float32),\n",
       " 'prince': array([0.6690954, 1.212434 ], dtype=float32),\n",
       " 'princess': array([0.8468873, 0.5226464], dtype=float32),\n",
       " 'queen': array([ 0.45890674, -0.7914091 ], dtype=float32),\n",
       " 'realm': array([ 0.43352002, -0.9279783 ], dtype=float32),\n",
       " 'royal': array([ 0.57931876, -1.0499104 ], dtype=float32),\n",
       " 'rule': array([ 0.82716703, -0.5857244 ], dtype=float32),\n",
       " 'son': array([-0.5071175,  1.6089387], dtype=float32),\n",
       " 'strong': array([-1.0395274,  1.5985358], dtype=float32),\n",
       " 'their': array([ 0.66103125, -0.9436734 ], dtype=float32),\n",
       " 'woman': array([-0.34705597, -0.08890723], dtype=float32)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4522477 ],\n",
       "       [-1.2182469 ],\n",
       "       [-0.2559153 ],\n",
       "       [ 1.1059917 ],\n",
       "       [-0.18299527],\n",
       "       [ 1.2638413 ],\n",
       "       [-0.37547705],\n",
       "       [ 0.23008995],\n",
       "       [-0.5719701 ],\n",
       "       [-0.95362383],\n",
       "       [-0.46083274],\n",
       "       [-0.5689876 ],\n",
       "       [-0.63507533],\n",
       "       [ 0.5916492 ],\n",
       "       [ 1.2399746 ],\n",
       "       [ 1.1080786 ],\n",
       "       [ 0.90944767],\n",
       "       [-1.4457749 ],\n",
       "       [-1.5976491 ],\n",
       "       [ 1.5208826 ],\n",
       "       [-0.28549275]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beautiful': 0,\n",
       " 'boy': 1,\n",
       " 'can': 2,\n",
       " 'children': 3,\n",
       " 'daughter': 4,\n",
       " 'family': 5,\n",
       " 'future': 6,\n",
       " 'king': 7,\n",
       " 'man': 8,\n",
       " 'now': 9,\n",
       " 'only': 10,\n",
       " 'prince': 11,\n",
       " 'princess': 12,\n",
       " 'queen': 13,\n",
       " 'realm': 14,\n",
       " 'royal': 15,\n",
       " 'rule': 16,\n",
       " 'son': 17,\n",
       " 'strong': 18,\n",
       " 'their': 19,\n",
       " 'woman': 20}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The future king is the prince',\n",
       " 'Daughter is the princess ',\n",
       " 'Son is the prince',\n",
       " 'Only a man can be a king ',\n",
       " 'Only a woman can be a queen',\n",
       " 'The princess will be a queen',\n",
       " 'Queen and king rule the realm',\n",
       " 'The prince is a strong man',\n",
       " 'The princess is a beautiful woman ',\n",
       " 'The royal family is the king and queen and their children',\n",
       " 'Prince is only a boy now',\n",
       " 'A boy will be a man']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[['future', 'king'], ['future', 'prince'], ['king', 'prince'], ['king', 'future'], \n",
    "  ['prince', 'king'], ['prince', 'future'], ['daughter', 'princess'], ['princess', 'daughter'],\n",
    "  ['son', 'prince'], ['prince', 'son'], ['only', 'man'], ['only', 'can'], ['only', 'king'], ['man', 'can'],\n",
    "  ['man', 'only'], ['man', 'king'], ['can', 'king'], ['can', 'man'], ['can', 'only'], ['king', 'can'], ['king', 'man'],\n",
    "  ['king', 'only'], ['only', 'woman'], ['only', 'can'], ['only', 'queen'], ['woman', 'can'], ['woman', 'only'], \n",
    "  ['woman', 'queen'], ['can', 'queen'], ['can', 'woman'], ['can', 'only'], ['queen', 'can'], ['queen', 'woman'], \n",
    "  ['queen', 'only'], ['princess', 'queen'], ['queen', 'princess'], ['queen', 'king'], ['queen', 'rule'],\n",
    "  ['queen', 'realm'], ['king', 'rule'], ['king', 'queen'], ['king', 'realm'], ['rule', 'realm'], \n",
    "  ['rule', 'king'], ['rule', 'queen'], ['realm', 'rule'], ['realm', 'king'], ['realm', 'queen'], ['prince', 'strong'], \n",
    "  ['prince', 'man'], ['strong', 'man'], ['strong', 'prince'], ['man', 'strong'], ['man', 'prince'], \n",
    "  ['princess', 'beautiful'], ['princess', 'woman'], ['beautiful', 'woman'], ['beautiful', 'princess'],\n",
    "  ['woman', 'beautiful'], ['woman', 'princess'], ['royal', 'family'], ['royal', 'king'], ['royal', 'queen'], \n",
    "  ['family', 'king'], ['family', 'royal'], ['family', 'queen'], ['family', 'their'], ['king', 'queen'], \n",
    "  ['king', 'family'], ['king', 'their'], ['king', 'royal'], ['king', 'children'], ['queen', 'their'], ['queen', 'king'],\n",
    "  ['queen', 'children'], ['queen', 'family'], ['queen', 'royal'], ['their', 'children'], ['their', 'queen'],\n",
    "  ['their', 'king'], ['their', 'family'], ['children', 'their'], ['children', 'queen'], ['children', 'king'],\n",
    "  ['prince', 'only'], ['prince', 'boy'], ['prince', 'now'], ['only', 'boy'], ['only', 'prince'], ['only', 'now'],\n",
    "  ['boy', 'now'], ['boy', 'only'], ['boy', 'prince'], ['now', 'boy'], ['now', 'only'], ['now', 'prince'], \n",
    "  ['boy', 'man'], ['man', 'boy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\win10\\appdata\\local\\pip\\cache\\wheels\\de\\5e\\42\\64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\\nltk-3.5-py3-none-any.whl\n",
      "Requirement already satisfied: regex in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from nltk) (2020.10.28)\n",
      "Requirement already satisfied: joblib in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages (from nltk) (4.43.0)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-e014e2ea6623>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "model = gensim.models.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-0536ea377d06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmovie_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtreebank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown, movie_reviews, treebank\n",
    "b = Word2Vec(brown.sents())\n",
    "b.vector_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "model = gensim.models.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('brown.embedding')\n",
    "new_model = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-e70e92d32c6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " [['future', 'king'], ['future', 'prince'], ['king', 'prince'], ['king', 'future'], \n",
    "  ['prince', 'king'], ['prince', 'future'], ['daughter', 'princess'], ['princess', 'daughter'],\n",
    "  ['son', 'prince'], ['prince', 'son'], ['only', 'man'], ['only', 'can'], ['only', 'king'], ['man', 'can'],\n",
    "  ['man', 'only'], ['man', 'king'], ['can', 'king'], ['can', 'man'], ['can', 'only'], ['king', 'can'], ['king', 'man'],\n",
    "  ['king', 'only'], ['only', 'woman'], ['only', 'can'], ['only', 'queen'], ['woman', 'can'], ['woman', 'only'], \n",
    "  ['woman', 'queen'], ['can', 'queen'], ['can', 'woman'], ['can', 'only'], ['queen', 'can'], ['queen', 'woman'], \n",
    "  ['queen', 'only'], ['princess', 'queen'], ['queen', 'princess'], ['queen', 'king'], ['queen', 'rule'],\n",
    "  ['queen', 'realm'], ['king', 'rule'], ['king', 'queen'], ['king', 'realm'], ['rule', 'realm'], \n",
    "  ['rule', 'king'], ['rule', 'queen'], ['realm', 'rule'], ['realm', 'king'], ['realm', 'queen'], ['prince', 'strong'], \n",
    "  ['prince', 'man'], ['strong', 'man'], ['strong', 'prince'], ['man', 'strong'], ['man', 'prince'], \n",
    "  ['princess', 'beautiful'], ['princess', 'woman'], ['beautiful', 'woman'], ['beautiful', 'princess'],\n",
    "  ['woman', 'beautiful'], ['woman', 'princess'], ['royal', 'family'], ['royal', 'king'], ['royal', 'queen'], \n",
    "  ['family', 'king'], ['family', 'royal'], ['family', 'queen'], ['family', 'their'], ['king', 'queen'], \n",
    "  ['king', 'family'], ['king', 'their'], ['king', 'royal'], ['king', 'children'], ['queen', 'their'], ['queen', 'king'],\n",
    "  ['queen', 'children'], ['queen', 'family'], ['queen', 'royal'], ['their', 'children'], ['their', 'queen'],\n",
    "  ['their', 'king'], ['their', 'family'], ['children', 'their'], ['children', 'queen'], ['children', 'king'],\n",
    "  ['prince', 'only'], ['prince', 'boy'], ['prince', 'now'], ['only', 'boy'], ['only', 'prince'], ['only', 'now'],\n",
    "  ['boy', 'now'], ['boy', 'only'], ['boy', 'prince'], ['now', 'boy'], ['now', 'only'], ['now', 'prince'], \n",
    "  ['boy', 'man'], ['man', 'boy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.2094697 , -1.7519492 , -0.58710396],\n",
       "        [-0.69742936, -0.8224115 ,  0.762515  ],\n",
       "        [-1.0232257 , -0.99347407, -0.6290375 ],\n",
       "        [-0.03148846,  0.861094  , -1.2489003 ],\n",
       "        [ 0.5185555 , -1.0197939 , -1.1642642 ],\n",
       "        [ 0.34528542,  0.4095648 , -1.4649962 ],\n",
       "        [-1.29276   ,  0.80572814, -0.03398667],\n",
       "        [ 0.40857503,  0.04271423, -0.2268797 ],\n",
       "        [-0.80741733,  0.8518701 ,  1.1676799 ],\n",
       "        [-1.5463994 ,  0.3928059 ,  0.8357471 ],\n",
       "        [-0.37743744,  0.01945627,  0.8162782 ],\n",
       "        [ 0.19316475, -0.22552669,  1.1549882 ],\n",
       "        [ 0.99186873, -0.91034657,  0.60148233],\n",
       "        [ 0.43138096, -0.24971391, -0.817542  ],\n",
       "        [-0.96682817,  0.11788289, -1.4357525 ],\n",
       "        [-0.37768555,  1.2125648 , -1.2955272 ],\n",
       "        [-0.24098323,  0.6833078 , -1.21673   ],\n",
       "        [-0.72854114,  0.79847366,  0.57885855],\n",
       "        [-0.8282897 , -0.26234597,  1.4050348 ],\n",
       "        [ 0.33650818,  1.3968124 , -1.0523984 ],\n",
       "        [ 0.08888734, -0.6616554 ,  0.04252516]], dtype=float32),\n",
       " array([-0.49878833,  0.2883682 , -0.38746664], dtype=float32),\n",
       " array([[ 8.6907446e-01, -6.6392583e-01, -4.6302193e-01,  1.5042695e+00,\n",
       "          5.5304325e-01,  5.6181812e-01,  4.8718947e-01, -9.5064050e-01,\n",
       "         -1.1298110e+00, -6.7061740e-01, -1.3256146e+00, -1.2928932e+00,\n",
       "          4.9400973e-01, -5.2003294e-01,  6.8872941e-01,  1.2350298e+00,\n",
       "         -3.3488446e-01,  8.8095433e-01,  1.2878020e-01,  1.2034481e+00,\n",
       "          3.4371424e-01],\n",
       "        [-1.0867966e+00,  9.0126878e-01,  3.5752165e-01,  7.8035134e-01,\n",
       "         -1.1255006e+00,  1.1815447e+00, -3.3572289e-01,  9.2761588e-01,\n",
       "         -1.2557857e+00, -1.1117984e+00, -7.5951183e-01,  7.3709536e-01,\n",
       "         -1.5203005e+00,  6.7338985e-01,  6.0285056e-01,  2.7194273e-01,\n",
       "          2.9193790e-04, -6.9741589e-01,  1.1521496e+00,  8.2621944e-01,\n",
       "         -1.0671973e+00],\n",
       "        [ 7.9353249e-01,  1.0735989e+00,  4.4674772e-01, -1.0618824e+00,\n",
       "          9.9174857e-01, -1.1751473e+00,  3.7053224e-01, -8.0935717e-01,\n",
       "          8.8709790e-01,  1.1756316e+00,  1.2205394e-01,  6.3423496e-01,\n",
       "         -1.0765996e+00, -9.5164514e-01, -9.4120181e-01, -1.3224217e+00,\n",
       "         -9.8728615e-01,  1.2041377e+00,  1.1767168e+00, -1.2862777e+00,\n",
       "         -2.0896141e-01]], dtype=float32),\n",
       " array([-0.04525664, -0.25900874,  0.5708084 , -0.37810788, -0.47273755,\n",
       "        -0.68486977, -0.17615764,  0.2708971 ,  0.23307966, -0.14422004,\n",
       "         0.43081766,  0.04142636, -0.13674738,  0.4116926 , -0.33597687,\n",
       "        -0.48062968, -0.5119752 , -0.3982598 , -0.3310347 , -0.4227993 ,\n",
       "         0.74141794], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2094697 , -1.7519492 , -0.58710396],\n",
       "       [-0.69742936, -0.8224115 ,  0.762515  ],\n",
       "       [-1.0232257 , -0.99347407, -0.6290375 ],\n",
       "       [-0.03148846,  0.861094  , -1.2489003 ],\n",
       "       [ 0.5185555 , -1.0197939 , -1.1642642 ],\n",
       "       [ 0.34528542,  0.4095648 , -1.4649962 ],\n",
       "       [-1.29276   ,  0.80572814, -0.03398667],\n",
       "       [ 0.40857503,  0.04271423, -0.2268797 ],\n",
       "       [-0.80741733,  0.8518701 ,  1.1676799 ],\n",
       "       [-1.5463994 ,  0.3928059 ,  0.8357471 ],\n",
       "       [-0.37743744,  0.01945627,  0.8162782 ],\n",
       "       [ 0.19316475, -0.22552669,  1.1549882 ],\n",
       "       [ 0.99186873, -0.91034657,  0.60148233],\n",
       "       [ 0.43138096, -0.24971391, -0.817542  ],\n",
       "       [-0.96682817,  0.11788289, -1.4357525 ],\n",
       "       [-0.37768555,  1.2125648 , -1.2955272 ],\n",
       "       [-0.24098323,  0.6833078 , -1.21673   ],\n",
       "       [-0.72854114,  0.79847366,  0.57885855],\n",
       "       [-0.8282897 , -0.26234597,  1.4050348 ],\n",
       "       [ 0.33650818,  1.3968124 , -1.0523984 ],\n",
       "       [ 0.08888734, -0.6616554 ,  0.04252516]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The future king is the prince',\n",
       " 'Daughter is the princess ',\n",
       " 'Son is the prince',\n",
       " 'Only a man can be a king ',\n",
       " 'Only a woman can be a queen',\n",
       " 'The princess will be a queen',\n",
       " 'Queen and king rule the realm',\n",
       " 'The prince is a strong man',\n",
       " 'The princess is a beautiful woman ',\n",
       " 'The royal family is the king and queen and their children',\n",
       " 'Prince is only a boy now',\n",
       " 'A boy will be a man']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the text from the input folder\n",
    "texts = pd.read_csv('input/sample.csv')\n",
    "texts = [x for x in texts['text']]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['future', 'king', 'prince']\n",
      "['daughter', 'princess']\n",
      "['son', 'prince']\n",
      "['only', 'man', 'can', 'king']\n",
      "['only', 'woman', 'can', 'queen']\n",
      "['princess', 'queen']\n",
      "['queen', 'king', 'rule', 'realm']\n",
      "['prince', 'strong', 'man']\n",
      "['princess', 'beautiful', 'woman']\n",
      "['royal', 'family', 'king', 'queen', 'their', 'children']\n",
      "['prince', 'only', 'boy', 'now']\n",
      "['boy', 'man']\n",
      "{'beautiful': 0, 'boy': 1, 'can': 2, 'children': 3, 'daughter': 4, 'family': 5, 'future': 6, 'king': 7, 'man': 8, 'now': 9, 'only': 10, 'prince': 11, 'princess': 12, 'queen': 13, 'realm': 14, 'royal': 15, 'rule': 16, 'son': 17, 'strong': 18, 'their': 19, 'woman': 20}\n"
     ]
    }
   ],
   "source": [
    "# Defining the window for context\n",
    "window = 2\n",
    "\n",
    "# Creating a placeholder for the scanning of the word list\n",
    "word_lists = []\n",
    "all_text = []\n",
    "\n",
    "for text in texts:\n",
    "\n",
    "    # Cleaning the text\n",
    "    text = text_preprocessing(text)\n",
    "    print(text)\n",
    "\n",
    "    # Appending to the all text list\n",
    "    all_text += text \n",
    "\n",
    "    # Creating a context dictionary\n",
    "    for i, word in enumerate(text):\n",
    "        for w in range(window):\n",
    "            # Getting the context that is ahead by *window* words\n",
    "            if i + 1 + w < len(text): \n",
    "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "            # Getting the context that is behind by *window* words    \n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists.append([word] + [text[(i - w - 1)]])\n",
    "\n",
    "unique_word_dict = create_unique_word_dict(all_text)\n",
    "\n",
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "\n",
    "# Getting all the unique words \n",
    "words = list(unique_word_dict.keys())\n",
    "\n",
    "# Creating the X and Y matrices using one hot encoding\n",
    "X = []\n",
    "Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['beautiful', 'boy', 'can', 'children', 'daughter', 'family', 'future', 'king', 'man', 'now', 'only', 'prince', 'princess', 'queen', 'realm', 'royal', 'rule', 'son', 'strong', 'their', 'woman'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_word_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['future', 'king'],\n",
       " ['future', 'prince'],\n",
       " ['king', 'prince'],\n",
       " ['king', 'future'],\n",
       " ['prince', 'king'],\n",
       " ['prince', 'future'],\n",
       " ['daughter', 'princess'],\n",
       " ['princess', 'daughter'],\n",
       " ['son', 'prince'],\n",
       " ['prince', 'son'],\n",
       " ['only', 'man'],\n",
       " ['only', 'can'],\n",
       " ['only', 'king'],\n",
       " ['man', 'can'],\n",
       " ['man', 'only'],\n",
       " ['man', 'king'],\n",
       " ['can', 'king'],\n",
       " ['can', 'man'],\n",
       " ['can', 'only'],\n",
       " ['king', 'can'],\n",
       " ['king', 'man'],\n",
       " ['king', 'only'],\n",
       " ['only', 'woman'],\n",
       " ['only', 'can'],\n",
       " ['only', 'queen'],\n",
       " ['woman', 'can'],\n",
       " ['woman', 'only'],\n",
       " ['woman', 'queen'],\n",
       " ['can', 'queen'],\n",
       " ['can', 'woman'],\n",
       " ['can', 'only'],\n",
       " ['queen', 'can'],\n",
       " ['queen', 'woman'],\n",
       " ['queen', 'only'],\n",
       " ['princess', 'queen'],\n",
       " ['queen', 'princess'],\n",
       " ['queen', 'king'],\n",
       " ['queen', 'rule'],\n",
       " ['queen', 'realm'],\n",
       " ['king', 'rule'],\n",
       " ['king', 'queen'],\n",
       " ['king', 'realm'],\n",
       " ['rule', 'realm'],\n",
       " ['rule', 'king'],\n",
       " ['rule', 'queen'],\n",
       " ['realm', 'rule'],\n",
       " ['realm', 'king'],\n",
       " ['realm', 'queen'],\n",
       " ['prince', 'strong'],\n",
       " ['prince', 'man'],\n",
       " ['strong', 'man'],\n",
       " ['strong', 'prince'],\n",
       " ['man', 'strong'],\n",
       " ['man', 'prince'],\n",
       " ['princess', 'beautiful'],\n",
       " ['princess', 'woman'],\n",
       " ['beautiful', 'woman'],\n",
       " ['beautiful', 'princess'],\n",
       " ['woman', 'beautiful'],\n",
       " ['woman', 'princess'],\n",
       " ['royal', 'family'],\n",
       " ['royal', 'king'],\n",
       " ['royal', 'queen'],\n",
       " ['royal', 'their'],\n",
       " ['royal', 'children'],\n",
       " ['family', 'king'],\n",
       " ['family', 'royal'],\n",
       " ['family', 'queen'],\n",
       " ['family', 'their'],\n",
       " ['family', 'children'],\n",
       " ['king', 'queen'],\n",
       " ['king', 'family'],\n",
       " ['king', 'their'],\n",
       " ['king', 'royal'],\n",
       " ['king', 'children'],\n",
       " ['queen', 'their'],\n",
       " ['queen', 'king'],\n",
       " ['queen', 'children'],\n",
       " ['queen', 'family'],\n",
       " ['queen', 'royal'],\n",
       " ['their', 'children'],\n",
       " ['their', 'queen'],\n",
       " ['their', 'king'],\n",
       " ['their', 'family'],\n",
       " ['their', 'royal'],\n",
       " ['children', 'their'],\n",
       " ['children', 'queen'],\n",
       " ['children', 'king'],\n",
       " ['children', 'family'],\n",
       " ['children', 'royal'],\n",
       " ['prince', 'only'],\n",
       " ['prince', 'boy'],\n",
       " ['prince', 'now'],\n",
       " ['only', 'boy'],\n",
       " ['only', 'prince'],\n",
       " ['only', 'now'],\n",
       " ['boy', 'now'],\n",
       " ['boy', 'only'],\n",
       " ['boy', 'prince'],\n",
       " ['now', 'boy'],\n",
       " ['now', 'only'],\n",
       " ['now', 'prince'],\n",
       " ['boy', 'man'],\n",
       " ['man', 'boy']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_word_index 6\n",
      "context_word_index 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "append not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-177ceb271a2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Appending to the main matrices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_row\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_row\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\win10\\anaconda3\\envs\\nlp_projects\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    685\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: append not found"
     ]
    }
   ],
   "source": [
    "for i, word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    main_word_index = unique_word_dict.get(word_list[0])\n",
    "    print(\"main_word_index\",main_word_index)\n",
    "    context_word_index = unique_word_dict.get(word_list[1])\n",
    "    print(\"context_word_index\",context_word_index)\n",
    "\n",
    "    # Creating the placeholders   \n",
    "    X_row = np.zeros(n_words)\n",
    "    Y_row = np.zeros(n_words)\n",
    "\n",
    "    # One hot encoding the main word\n",
    "    X_row[main_word_index] = 1\n",
    "\n",
    "    # One hot encoding the Y matrix words \n",
    "    Y_row[context_word_index] = 1\n",
    "\n",
    "    # Appending to the main matrices\n",
    "    X.append(X_row)\n",
    "    Y.append(Y_row)\n",
    "\n",
    "# Converting the matrices into a sparse format because the vast majority of the data are 0s\n",
    "X = sparse.csr_matrix(X)\n",
    "Y = sparse.csr_matrix(Y)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_row = np.zeros(n_words)\n",
    "X_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48290667,  0.8457001 ,  0.59346896,  1.2645154 , -1.2040269 ,\n",
       "         1.1573222 ],\n",
       "       [ 0.73837304, -0.20747106,  0.4157775 , -1.1825156 ,  1.3830478 ,\n",
       "        -0.05894116],\n",
       "       [-0.5129488 , -0.6455666 ,  0.22340234, -0.2326535 ,  0.6847749 ,\n",
       "         0.5590865 ],\n",
       "       [-0.6766587 , -0.7023404 , -1.1072837 ,  0.59157366, -0.8064974 ,\n",
       "        -0.5534737 ],\n",
       "       [-0.8892283 ,  0.96314925, -0.19587861,  1.0056496 , -1.0038648 ,\n",
       "         1.3950247 ],\n",
       "       [-0.5153948 , -1.0223849 , -1.188952  ,  0.27847248, -0.77112186,\n",
       "        -0.8921992 ],\n",
       "       [ 1.2034631 , -1.1523337 , -1.4931266 ,  0.46258503,  0.99452   ,\n",
       "        -1.2687634 ],\n",
       "       [-1.1228006 ,  0.60329014, -0.13347368, -1.2324024 , -0.12903307,\n",
       "        -0.8778439 ],\n",
       "       [ 1.0579053 , -1.3727962 ,  0.79554003,  0.38659805,  0.41290584,\n",
       "        -0.20482758],\n",
       "       [ 1.0717387 , -0.9128558 ,  0.52586603, -0.40760133,  0.89914244,\n",
       "         0.43862754],\n",
       "       [ 0.58207464, -0.9047044 ,  0.25423622, -0.38664833, -0.13059032,\n",
       "         0.1812161 ],\n",
       "       [ 0.476674  ,  0.12483631,  0.95619524, -0.06299967,  1.0470625 ,\n",
       "        -1.2895753 ],\n",
       "       [ 0.97549105,  1.1289161 , -0.54957724, -0.94887674, -0.87451756,\n",
       "         1.1080424 ],\n",
       "       [-1.0478702 ,  0.29631495,  0.6564594 ,  0.7506916 , -0.2896547 ,\n",
       "        -0.3990863 ],\n",
       "       [-0.7447832 ,  0.7770811 , -0.7349353 ,  0.92941946,  1.2602828 ,\n",
       "         0.8298819 ],\n",
       "       [-1.2512238 , -0.39256236, -0.9883344 ,  0.23578405, -0.3035798 ,\n",
       "        -0.8775306 ],\n",
       "       [-0.36981118,  0.97570264, -1.281236  ,  0.8374807 ,  1.1327404 ,\n",
       "        -0.23229918],\n",
       "       [ 0.97142696, -1.0228343 , -0.83191454, -0.9423956 ,  0.94344723,\n",
       "        -0.8568555 ],\n",
       "       [ 0.47624534, -1.1111679 , -0.12512505, -1.1358008 ,  0.84627765,\n",
       "        -1.3412069 ],\n",
       "       [-0.5601015 , -0.36370665, -0.7968366 ,  0.6025755 , -0.5444036 ,\n",
       "        -0.82952034],\n",
       "       [-0.6789324 ,  0.42493168,  0.19000857, -1.203751  , -0.86895406,\n",
       "         1.2139951 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
